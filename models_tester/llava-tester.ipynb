{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12457166,"sourceType":"datasetVersion","datasetId":7858089},{"sourceId":12457216,"sourceType":"datasetVersion","datasetId":7858132},{"sourceId":12657249,"sourceType":"datasetVersion","datasetId":7998846},{"sourceId":12657280,"sourceType":"datasetVersion","datasetId":7998865},{"sourceId":12658472,"sourceType":"datasetVersion","datasetId":7999569},{"sourceId":12924296,"sourceType":"datasetVersion","datasetId":8178199},{"sourceId":12928267,"sourceType":"datasetVersion","datasetId":8180731},{"sourceId":12943779,"sourceType":"datasetVersion","datasetId":8191200}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install git+https://github.com/huggingface/transformers accelerate qwen-vl-utils bitsandbytes","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-24T10:03:34.554665Z","iopub.execute_input":"2025-07-24T10:03:34.554989Z","iopub.status.idle":"2025-07-24T10:05:26.921043Z","shell.execute_reply.started":"2025-07-24T10:03:34.554965Z","shell.execute_reply":"2025-07-24T10:05:26.920319Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T16:43:32.021870Z","iopub.execute_input":"2025-07-16T16:43:32.022667Z","iopub.status.idle":"2025-07-16T16:43:35.088185Z","shell.execute_reply.started":"2025-07-16T16:43:32.022603Z","shell.execute_reply":"2025-07-16T16:43:35.087501Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import Dataset\n\n# Percorso del file salvato\n# json_path = \"/kaggle/input/final-test-set/final_dataset.json\"\ndataset_path = '/kaggle/input/summarized-dataset/dataset_summarized'\n\n# Carica il dataset Hugging Face\n# hf_dataset = Dataset.from_json(json_path)\nhf_dataset = Dataset.load_from_disk(dataset_path)\n\n# Controlla un esempio\nprint(hf_dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T10:05:33.157576Z","iopub.execute_input":"2025-07-24T10:05:33.158310Z","iopub.status.idle":"2025-07-24T10:05:34.822292Z","shell.execute_reply.started":"2025-07-24T10:05:33.158273Z","shell.execute_reply":"2025-07-24T10:05:34.821736Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import pipeline\n\n# Carica il classificatore zero-shot\nclassifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n\n# Etichette da prevedere\nimport re\n\nLABELS = [\"real\", \"fake\"]\n\ndef extract_label_from_response(response: str) -> int:\n    result = classifier(response, LABELS)\n    return LABELS.index(result[\"labels\"][0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T10:05:36.110935Z","iopub.execute_input":"2025-07-24T10:05:36.111326Z","iopub.status.idle":"2025-07-24T10:06:10.378527Z","shell.execute_reply.started":"2025-07-24T10:05:36.111305Z","shell.execute_reply":"2025-07-24T10:06:10.377723Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import (\n    LlavaNextProcessor,\n    LlavaNextForConditionalGeneration,\n    BitsAndBytesConfig,\n    GenerationConfig\n)\nfrom PIL import Image\nimport torch\nimport os\n\nos.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n\n# Few-shot esempi per real/fake\nFEW_SHOT_EXAMPLES = [\n    {\n        \"context\": \"A street photo taken at midday with perfectly natural shadows and realistic reflections on wet pavement.\",\n        \"label\": \"[Real]\",\n        \"motivation\": \"Consistent lighting and natural shadow falloff; reflections are coherent with the surfaces.\"\n    },\n    {\n        \"context\": \"An indoor scene where the textures on the walls appear overly smooth and uniform, and shadows lack realistic variation.\",\n        \"label\": \"[Fake]\",\n        \"motivation\": \"Uniform texture and flat shadows are typical AI generation artifacts.\"\n    },\n]\n\nSTATIC_PROMPTS = [\n    \"Is this image real or fake? Choose one: [Real] / [Fake].\",\n    \"Analyze and classify: [Real] / [Fake].\",\n    \"Based on lighting and texture, decide: [Real] / [Fake].\"\n]\n\nclass LlavaTester:\n    def __init__(self, model_id=\"llava-hf/llava-v1.6-mistral-7b-hf\", device=\"cuda:0\"):\n        self.device = torch.device(device)\n        self.processor = LlavaNextProcessor.from_pretrained(model_id)\n\n        self.model = LlavaNextForConditionalGeneration.from_pretrained(\n            model_id, \n            torch_dtype=torch.float16, \n            low_cpu_mem_usage=True,\n            load_in_4bit=True\n        )\n\n    def build_prompt(self, prompt_type=\"dynamic\", x_t=\"\"):\n        few_shot = \"\".join(\n            f\"Example:\\nContext: {ex['context']}\\nAnswer: {ex['label']} Motivation: {ex['motivation']}\\n\\n\"\n            for ex in FEW_SHOT_EXAMPLES\n        )\n        if prompt_type == \"dynamic\" and x_t:\n            return (\n                \"You are a classifier that labels images as [Real] or [Fake].\\n\"\n                \"Analyze the following description and the image, return ONLY one label: [Real] or [Fake]. Do not explain.\\n\\n\"\n                f\"Image Description and Technical Analysis:\\n{x_t.strip()}\\n\\n\"\n                \"Answer with exactly one of the two options:\\n[Real]\\n[Fake]\\n\\n\"\n                \"Answer:\"\n            )\n        else:\n            idx = int(prompt_type.split(\"_\")[-1]) if \"_\" in prompt_type else 0\n            template = STATIC_PROMPTS[idx]\n            return few_shot + f\"Question: {template}\\nAnswer:\"\n\n    def generate(self, image_path: str, prompt: str) -> str:\n        # 1) load+resize\n        img = Image.open(image_path).convert(\"RGB\")\n        \n\n        # 2) build chat\n        chat = self.processor.apply_chat_template(\n            [{\"role\":\"user\",\"content\":[{\"type\":\"text\",\"text\":prompt},{\"type\":\"image\"}]}],\n            add_generation_prompt=True\n        )\n\n        # 3) tokenize + image\n        inputs = self.processor(text=chat, images=img, return_tensors=\"pt\", padding=True).to(self.device)\n\n        # 4) generate\n        out = self.model.generate(\n            **inputs,\n            max_new_tokens=100,\n        )\n\n        response = self.processor.decode(out[0], skip_special_tokens=True).strip()\n\n        # Estrai solo la risposta finale, ignorando i few-shot examples\n        # Trova l'ultima occorrenza di \"Answer:\" e restituisci la parte successiva\n        answer_start = response.rfind(\"Answer:\")  # Trova l'ultima occorrenza di \"Answer:\"\n        if answer_start != -1:\n            response = response[answer_start:].split(\"\\n\")[-1].strip()  # Ottieni solo la parte dopo \"Answer:\"\n        return response\n\n    def extract_label(self, response: str) -> int:\n        return extract_label_from_response(response)\n\n    def test(self, example: dict, prompt_type=\"dynamic\"):\n        torch.cuda.empty_cache()\n        prompt = self.build_prompt(prompt_type, example.get(\"summarized_x_t\",\"\"))\n        resp = self.generate(example[\"image_k\"], prompt)\n        label = self.extract_label(resp)\n        return label, resp\n\n# wrapper\n_tester = LlavaTester()\ndef test_llava(example, prompt_type=\"dynamic\"):\n    return _tester.test(example, prompt_type)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T10:13:06.151514Z","iopub.execute_input":"2025-07-24T10:13:06.152187Z","iopub.status.idle":"2025-07-24T10:14:12.468537Z","shell.execute_reply.started":"2025-07-24T10:13:06.152161Z","shell.execute_reply":"2025-07-24T10:14:12.467828Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import load_from_disk\nfrom tqdm import tqdm\nimport pandas as pd\n\ndef run_test():\n    #dataset = load_from_disk(DATASET_PATH)\n    for i in range(10):\n        #print(\"Dataset structure:\\n\", hf_dataset)\n        example = hf_dataset[i]\n        print(\"Ground_Truth:\\n\", example['label'])\n        #print(\"Image:\\n\", example['image'])\n        #print(\"Prompt:\\n\", example['x_t'])\n        label, response = test_llava(example, \"dynamic\")\n        print(\"Output label:\\n\", label)\n        print(\"Output response:\\n\", response)\n\n#run_test()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T10:14:29.591818Z","iopub.execute_input":"2025-07-24T10:14:29.592691Z","iopub.status.idle":"2025-07-24T10:17:21.495917Z","shell.execute_reply.started":"2025-07-24T10:14:29.592662Z","shell.execute_reply":"2025-07-24T10:17:21.495062Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport pandas as pd\nfrom tqdm.auto import tqdm\nfrom datasets import load_from_disk\n\n# DATASET_PATH   = \"/kaggle/input/test-dataset-kaggle/test_dataset_kaggle\"\nCHECKPOINT_CSV = \"/kaggle/working/dynamic_new_dataset_sum_llava.csv\"\nPROMPT_TYPE    = \"dynamic\"\nCHECKPOINT_EVERY = 30  # salva ogni 30 campioni\n\n# 1) Carica dataset\n#dataset = load_from_disk(DATASET_PATH)\n\n# 2) Se esiste già un CSV di checkpoint, riloadalo e salta i campioni già processati\nif os.path.exists(CHECKPOINT_CSV):\n    df = pd.read_csv(CHECKPOINT_CSV)\n    processed_ids = set(df[\"img_id\"])\nelse:\n    df = pd.DataFrame(columns=[\"img_id\",\"gt_label\",\"pred_label\",\"response\"])\n    processed_ids = set()\n\n# 3) Loop con tqdm e checkpoint ogni N campioni\nfor sample in tqdm(hf_dataset, desc=f\"Eval {PROMPT_TYPE}\", dynamic_ncols=True):\n    img_id = sample[\"img_id\"]\n    if img_id in processed_ids:\n        continue\n\n    try:\n        pred_label, resp = test_llava(sample, prompt_type=PROMPT_TYPE)\n    except Exception as e:\n        print(\"Errore nella generazione\")\n        pred_label, resp = -1, f\"[ERROR] {e}\"\n\n    # prepara la nuova riga\n    row = {\n        \"img_id\":     img_id,\n        \"gt_label\":   sample[\"label\"],\n        \"pred_label\": pred_label,\n        \"response\":   resp\n    }\n    # concatena in un colpo solo\n    df = pd.concat([df, pd.DataFrame([row])], ignore_index=True)\n    processed_ids.add(img_id)\n\n    # salva checkpoint\n    if len(processed_ids) % CHECKPOINT_EVERY == 0:\n        df.to_csv(CHECKPOINT_CSV, index=False)\n\n# 4) Alla fine salva il CSV definitivo\ndf.to_csv(CHECKPOINT_CSV, index=False)\nprint(\"✅ Checkpoint salvato in:\", CHECKPOINT_CSV)\n\n# 5) Calcolo accuracy su quelli validi\nvalid = df[\"pred_label\"] != -1\nacc = (df.loc[valid, \"gt_label\"] == df.loc[valid, \"pred_label\"]).mean()\nprint(f\"Accuracy {PROMPT_TYPE}: {acc:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}