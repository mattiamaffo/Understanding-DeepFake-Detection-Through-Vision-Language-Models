{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12457166,"sourceType":"datasetVersion","datasetId":7858089},{"sourceId":12457216,"sourceType":"datasetVersion","datasetId":7858132},{"sourceId":12657249,"sourceType":"datasetVersion","datasetId":7998846},{"sourceId":12657280,"sourceType":"datasetVersion","datasetId":7998865},{"sourceId":12658472,"sourceType":"datasetVersion","datasetId":7999569},{"sourceId":12924296,"sourceType":"datasetVersion","datasetId":8178199},{"sourceId":12928267,"sourceType":"datasetVersion","datasetId":8180731},{"sourceId":12943779,"sourceType":"datasetVersion","datasetId":8191200},{"sourceId":13135809,"sourceType":"datasetVersion","datasetId":8321839},{"sourceId":13190100,"sourceType":"datasetVersion","datasetId":8358840},{"sourceId":13242298,"sourceType":"datasetVersion","datasetId":8390713},{"sourceId":13242428,"sourceType":"datasetVersion","datasetId":8390805},{"sourceId":13242481,"sourceType":"datasetVersion","datasetId":8390843},{"sourceId":13248061,"sourceType":"datasetVersion","datasetId":8394451},{"sourceId":13248073,"sourceType":"datasetVersion","datasetId":8394461},{"sourceId":13248132,"sourceType":"datasetVersion","datasetId":8394510}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install git+https://github.com/huggingface/transformers accelerate qwen-vl-utils bitsandbytes","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true,"jupyter":{"outputs_hidden":true},"execution":{"iopub.status.busy":"2025-10-15T11:32:59.909767Z","iopub.execute_input":"2025-10-15T11:32:59.910015Z","iopub.status.idle":"2025-10-15T11:35:02.848335Z","shell.execute_reply.started":"2025-10-15T11:32:59.909989Z","shell.execute_reply":"2025-10-15T11:35:02.847283Z"}},"outputs":[{"name":"stdout","text":"Collecting git+https://github.com/huggingface/transformers\n  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-uwb929k5\n  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-uwb929k5\n  Resolved https://github.com/huggingface/transformers to commit e2122c4bcb74d942bb93c11dcb55aafc4c7fdf23\n  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.8.1)\nCollecting qwen-vl-utils\n  Downloading qwen_vl_utils-0.0.14-py3-none-any.whl.metadata (9.0 kB)\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.48.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==5.0.0.dev0) (3.18.0)\nCollecting huggingface-hub==1.0.0.rc5 (from transformers==5.0.0.dev0)\n  Downloading huggingface_hub-1.0.0rc5-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==5.0.0.dev0) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==5.0.0.dev0) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==5.0.0.dev0) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==5.0.0.dev0) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==5.0.0.dev0) (2.32.4)\nCollecting tokenizers<=0.23.0,>=0.22.0 (from transformers==5.0.0.dev0)\n  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers==5.0.0.dev0) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==5.0.0.dev0) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub==1.0.0.rc5->transformers==5.0.0.dev0) (2025.5.1)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub==1.0.0.rc5->transformers==5.0.0.dev0) (0.28.1)\nCollecting typer-slim (from huggingface-hub==1.0.0.rc5->transformers==5.0.0.dev0)\n  Downloading typer_slim-0.19.2-py3-none-any.whl.metadata (16 kB)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub==1.0.0.rc5->transformers==5.0.0.dev0) (4.14.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub==1.0.0.rc5->transformers==5.0.0.dev0) (1.1.5)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (7.0.0)\nRequirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\nCollecting av (from qwen-vl-utils)\n  Downloading av-16.0.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.6 kB)\nRequirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from qwen-vl-utils) (11.2.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==5.0.0.dev0) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==5.0.0.dev0) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==5.0.0.dev0) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==5.0.0.dev0) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==5.0.0.dev0) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==5.0.0.dev0) (2.4.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==5.0.0.dev0) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==5.0.0.dev0) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==5.0.0.dev0) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==5.0.0.dev0) (2025.6.15)\nRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->huggingface-hub==1.0.0.rc5->transformers==5.0.0.dev0) (4.9.0)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->huggingface-hub==1.0.0.rc5->transformers==5.0.0.dev0) (1.0.9)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub==1.0.0.rc5->transformers==5.0.0.dev0) (0.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers==5.0.0.dev0) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers==5.0.0.dev0) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers==5.0.0.dev0) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers==5.0.0.dev0) (2024.2.0)\nRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer-slim->huggingface-hub==1.0.0.rc5->transformers==5.0.0.dev0) (8.2.1)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers==5.0.0.dev0) (2024.2.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->huggingface-hub==1.0.0.rc5->transformers==5.0.0.dev0) (1.3.1)\nDownloading huggingface_hub-1.0.0rc5-py3-none-any.whl (501 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m502.0/502.0 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading qwen_vl_utils-0.0.14-py3-none-any.whl (8.1 kB)\nDownloading bitsandbytes-0.48.1-py3-none-manylinux_2_24_x86_64.whl (60.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.1/60.1 MB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m79.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m94.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m72.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m82.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading av-16.0.1-cp311-cp311-manylinux_2_28_x86_64.whl (40.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.2/40.2 MB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading typer_slim-0.19.2-py3-none-any.whl (46 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.8/46.8 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: transformers\n  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for transformers: filename=transformers-5.0.0.dev0-py3-none-any.whl size=11394600 sha256=63e51a047de71154df051f8cade8597c445471490bd213deb294b117483ed9ed\n  Stored in directory: /tmp/pip-ephem-wheel-cache-2alpapl4/wheels/04/a3/f1/b88775f8e1665827525b19ac7590250f1038d947067beba9fb\nSuccessfully built transformers\nInstalling collected packages: typer-slim, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, av, qwen-vl-utils, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, huggingface-hub, tokenizers, transformers, bitsandbytes\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n  Attempting uninstall: huggingface-hub\n    Found existing installation: huggingface-hub 0.33.1\n    Uninstalling huggingface-hub-0.33.1:\n      Successfully uninstalled huggingface-hub-0.33.1\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.21.2\n    Uninstalling tokenizers-0.21.2:\n      Successfully uninstalled tokenizers-0.21.2\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.52.4\n    Uninstalling transformers-4.52.4:\n      Successfully uninstalled transformers-4.52.4\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.5.1 which is incompatible.\nsentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 5.0.0.dev0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed av-16.0.1 bitsandbytes-0.48.1 huggingface-hub-1.0.0rc5 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 qwen-vl-utils-0.0.14 tokenizers-0.22.1 transformers-5.0.0.dev0 typer-slim-0.19.2\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"pip install -U scikit-learn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T11:35:16.650008Z","iopub.execute_input":"2025-10-15T11:35:16.650295Z","iopub.status.idle":"2025-10-15T11:35:23.940268Z","shell.execute_reply.started":"2025-10-15T11:35:16.650256Z","shell.execute_reply":"2025-10-15T11:35:23.939318Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.2.2)\nCollecting scikit-learn\n  Downloading scikit_learn-1.7.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\nRequirement already satisfied: numpy>=1.22.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)\nRequirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\nRequirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.0->scikit-learn) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.0->scikit-learn) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.0->scikit-learn) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.0->scikit-learn) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.0->scikit-learn) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.0->scikit-learn) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.22.0->scikit-learn) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.22.0->scikit-learn) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.22.0->scikit-learn) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.22.0->scikit-learn) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.22.0->scikit-learn) (2024.2.0)\nDownloading scikit_learn-1.7.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m66.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: scikit-learn\n  Attempting uninstall: scikit-learn\n    Found existing installation: scikit-learn 1.2.2\n    Uninstalling scikit-learn-1.2.2:\n      Successfully uninstalled scikit-learn-1.2.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncategory-encoders 2.7.0 requires scikit-learn<1.6.0,>=1.0.0, but you have scikit-learn 1.7.2 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nsklearn-compat 0.1.3 requires scikit-learn<1.7,>=1.2, but you have scikit-learn 1.7.2 which is incompatible.\nsentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 5.0.0.dev0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed scikit-learn-1.7.2\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# ================== CONFIG ==================\nINPUT_DIR     = \"/kaggle/input/qwen-csv/Qwen_test\"  # cartella che contiene i .csv\nY_TRUE_COL    = \"gt_label\"                            # cambia se serve\nY_PRED_COL    = \"pred_label\"                          # cambia se serve\nOUTPUT_DIR    = \"/kaggle/working/qwen_test\"                     # cartella di output\nFILE_PATTERN  = \"*.csv\"                               # pattern file da processare\n# ============================================\n\nimport os\nimport glob\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import (\n    accuracy_score,\n    precision_recall_fscore_support,\n    confusion_matrix,\n    ConfusionMatrixDisplay,\n)\n\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\ndef _norm(s: pd.Series) -> pd.Series:\n    \"\"\"Trim + uppercase stringhe; lascia int/float inalterati.\"\"\"\n    if s.dtype == object:\n        return s.astype(str).str.strip().replace({\"\": np.nan}).str.upper()\n    return s\n\ndef evaluate_one(csv_path: str, out_dir: str, y_true_col: str, y_pred_col: str):\n    df = pd.read_csv(csv_path)\n\n    if y_true_col not in df.columns or y_pred_col not in df.columns:\n        raise ValueError(\n            f\"Colonne non trovate in {os.path.basename(csv_path)}. \"\n            f\"Disponibili: {list(df.columns)}\"\n        )\n\n    y_true = _norm(df[y_true_col])\n    y_pred = _norm(df[y_pred_col])\n\n    # rimuovi righe con NaN\n    mask = y_true.notna() & y_pred.notna()\n    dropped = int((~mask).sum())\n    if dropped > 0:\n        print(f\"[{os.path.basename(csv_path)}] Righe scartate per NaN: {dropped}\")\n    y_true = y_true[mask]\n    y_pred = y_pred[mask]\n\n    # etichette\n    labels = sorted(pd.Index(y_true.unique()).union(y_pred.unique()).tolist())\n\n    # metriche globali\n    acc = accuracy_score(y_true, y_pred)\n    p_micro, r_micro, f1_micro, _ = precision_recall_fscore_support(y_true, y_pred, average=\"micro\", zero_division=0)\n    p_macro, r_macro, f1_macro, _ = precision_recall_fscore_support(y_true, y_pred, average=\"macro\", zero_division=0)\n    p_weighted, r_weighted, f1_weighted, _ = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\", zero_division=0)\n\n    # metriche per classe\n    p_c, r_c, f1_c, sup_c = precision_recall_fscore_support(\n        y_true, y_pred, labels=labels, average=None, zero_division=0\n    )\n\n    # confusion matrix\n    cm = confusion_matrix(y_true, y_pred, labels=labels)\n\n    # costruisci unico CSV (long)\n    rows = [\n        {\"scope\":\"summary\",\"class\":\"\",\"metric\":\"accuracy\",\"value\":acc},\n        {\"scope\":\"summary\",\"class\":\"\",\"metric\":\"precision_micro\",\"value\":p_micro},\n        {\"scope\":\"summary\",\"class\":\"\",\"metric\":\"recall_micro\",\"value\":r_micro},\n        {\"scope\":\"summary\",\"class\":\"\",\"metric\":\"f1_micro\",\"value\":f1_micro},\n        {\"scope\":\"summary\",\"class\":\"\",\"metric\":\"precision_macro\",\"value\":p_macro},\n        {\"scope\":\"summary\",\"class\":\"\",\"metric\":\"recall_macro\",\"value\":r_macro},\n        {\"scope\":\"summary\",\"class\":\"\",\"metric\":\"f1_macro\",\"value\":f1_macro},\n        {\"scope\":\"summary\",\"class\":\"\",\"metric\":\"precision_weighted\",\"value\":p_weighted},\n        {\"scope\":\"summary\",\"class\":\"\",\"metric\":\"recall_weighted\",\"value\":r_weighted},\n        {\"scope\":\"summary\",\"class\":\"\",\"metric\":\"f1_weighted\",\"value\":f1_weighted},\n    ]\n    for c, p, r, f1, sup in zip(labels, p_c, r_c, f1_c, sup_c):\n        rows += [\n            {\"scope\":\"per-class\",\"class\":str(c),\"metric\":\"precision\",\"value\":p},\n            {\"scope\":\"per-class\",\"class\":str(c),\"metric\":\"recall\",\"value\":r},\n            {\"scope\":\"per-class\",\"class\":str(c),\"metric\":\"f1\",\"value\":f1},\n            {\"scope\":\"per-class\",\"class\":str(c),\"metric\":\"support\",\"value\":sup},\n        ]\n    metrics_df = pd.DataFrame(rows)\n\n    # nomi coerenti con l'input\n    stem = os.path.splitext(os.path.basename(csv_path))[0]\n    metrics_csv_path = os.path.join(out_dir, f\"{stem}_metrics.csv\")\n    cm_png_path      = os.path.join(out_dir, f\"{stem}_confusion_matrix.png\")\n\n    metrics_df.to_csv(metrics_csv_path, index=False)\n\n    # salva immagine CM\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n    disp.plot(values_format=\"d\")\n    plt.title(\"Confusion Matrix\")\n    \n    # usa la figura restituita dal display\n    disp.figure_.tight_layout()\n    disp.figure_.savefig(cm_png_path, dpi=200)\n    plt.close(disp.figure_)\n\n    print(f\"[OK] {stem}: metrics -> {metrics_csv_path}\")\n    print(f\"[OK] {stem}: confusion matrix -> {cm_png_path}\")\n\n# ---- LOOP sui CSV ----\ncsv_files = sorted(glob.glob(os.path.join(INPUT_DIR, FILE_PATTERN)))\nif not csv_files:\n    print(f\"Nessun file trovato in {INPUT_DIR} con pattern {FILE_PATTERN}\")\nelse:\n    print(f\"Trovati {len(csv_files)} file CSV. Avvio calcolo metriche...\")\n    for csvf in csv_files:\n        try:\n            evaluate_one(csvf, OUTPUT_DIR, Y_TRUE_COL, Y_PRED_COL)\n        except Exception as e:\n            print(f\"[ERRORE] {os.path.basename(csvf)}: {e}\")\n    print(\"Completato.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T11:42:55.145960Z","iopub.execute_input":"2025-10-15T11:42:55.146278Z","iopub.status.idle":"2025-10-15T11:42:55.315994Z","shell.execute_reply.started":"2025-10-15T11:42:55.146246Z","shell.execute_reply":"2025-10-15T11:42:55.315158Z"}},"outputs":[{"name":"stdout","text":"Trovati 9 file CSV. Avvio calcolo metriche...\n[ERRORE] dynamic_checkpoint_qwen.csv: At least one label specified must be in y_true\n[ERRORE] dynamic_checkpoint_qwen_blurred.csv: At least one label specified must be in y_true\n[ERRORE] dynamic_checkpoint_qwen_shuffled.csv: At least one label specified must be in y_true\n[ERRORE] dynamic_new_dataset_Qwen.csv: At least one label specified must be in y_true\n[ERRORE] dynamic_test_Qwen_finetuned_2.csv: At least one label specified must be in y_true\n[ERRORE] static_0_checkpoint_qwen.csv: At least one label specified must be in y_true\n[ERRORE] static_0_checkpoint_qwen_blurred.csv: At least one label specified must be in y_true\n[ERRORE] static_0_new_dataset_Qwen.csv: At least one label specified must be in y_true\n[ERRORE] static_test_Qwen_finetuned_2.csv: At least one label specified must be in y_true\nCompletato.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"!zip -r /kaggle/working/results_idefics.zip /kaggle/working/idefics_test\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom datasets import load_from_disk\nfrom PIL import Image\n\n# Percorso dataset\ndataset_path = \"/kaggle/input/new-correct-dataset-2/kaggle/working/new_correct_dataset_2\"\n\n# Cartelle di output\noutput_dir = \"/kaggle/working/test_images_2\"\nfake_dir = os.path.join(output_dir, \"fake_images\")\nreal_dir = os.path.join(output_dir, \"real_images\")\n\nos.makedirs(fake_dir, exist_ok=True)\nos.makedirs(real_dir, exist_ok=True)\n\n# Numero massimo di immagini da salvare per classe\nmax_per_class = 4\n\n# Carica il dataset\ndataset = load_from_disk(dataset_path)\n\n# Contatori\nfake_count = 0\nreal_count = 0\n\nfor sample in dataset:\n    label = sample['label']\n    img_path = sample[\"image_k\"]  # assuming image_ssh contiene il path dell'immagine\n\n    # Salva solo se non abbiamo superato il massimo\n    if label == 1 and fake_count < max_per_class:\n        print(\"Sono qui fake\")\n        image = Image.open(img_path).convert(\"RGB\")\n        save_path = os.path.join(fake_dir, f\"fake_{fake_count+1}.png\")\n        image.save(save_path)\n        fake_count += 1\n\n    elif label == 0 and real_count < max_per_class:\n        print(\"Sono qui real\")\n        image = Image.open(img_path).convert(\"RGB\")\n        save_path = os.path.join(real_dir, f\"real_{real_count+1}.png\")\n        image.save(save_path)\n        real_count += 1\n\n    # Termina se abbiamo raccolto tutte le immagini necessarie\n    if fake_count >= max_per_class and real_count >= max_per_class:\n        break\n\nprint(f\"Salvate {fake_count} immagini FAKE in {fake_dir}\")\nprint(f\"Salvate {real_count} immagini REAL in {real_dir}\")\nprint(f\"Tutte le immagini salvate in: {output_dir}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-01T06:51:42.851851Z","iopub.execute_input":"2025-10-01T06:51:42.852541Z","iopub.status.idle":"2025-10-01T06:51:44.821649Z","shell.execute_reply.started":"2025-10-01T06:51:42.852501Z","shell.execute_reply":"2025-10-01T06:51:44.820910Z"}},"outputs":[{"name":"stdout","text":"Sono qui real\nSono qui fake\nSono qui fake\nSono qui real\nSono qui real\nSono qui real\nSono qui fake\nSono qui fake\nSalvate 4 immagini FAKE in /kaggle/working/test_images_2/fake_images\nSalvate 4 immagini REAL in /kaggle/working/test_images_2/real_images\nTutte le immagini salvate in: /kaggle/working/test_images_2\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"!zip -r /kaggle/working/attn_insights_2.zip /kaggle/working/attn_insights\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T08:03:06.115631Z","iopub.execute_input":"2025-10-03T08:03:06.115926Z","iopub.status.idle":"2025-10-03T08:03:06.871322Z","shell.execute_reply.started":"2025-10-03T08:03:06.115908Z","shell.execute_reply":"2025-10-03T08:03:06.870607Z"}},"outputs":[{"name":"stdout","text":"  adding: kaggle/working/attn_insights/ (stored 0%)\n  adding: kaggle/working/attn_insights/sample703_dynamic_attn_tokens.csv (deflated 56%)\n  adding: kaggle/working/attn_insights/sample98_dynamic_attn_prompt.png (deflated 14%)\n  adding: kaggle/working/attn_insights/sample25_dynamic_attn_alltokens.png (deflated 16%)\n  adding: kaggle/working/attn_insights/sample703_dynamic_attn_prompt.png (deflated 14%)\n  adding: kaggle/working/attn_insights/sample931_dynamic_attn_alltokens.png","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":" (deflated 17%)\n  adding: kaggle/working/attn_insights/sample324_dynamic_attn_alltokens.png (deflated 16%)\n  adding: kaggle/working/attn_insights/sample347_dynamic_attn_prompt.png (deflated 14%)\n  adding: kaggle/working/attn_insights/sample790_dynamic_attn_tokens.csv (deflated 55%)\n  adding: kaggle/working/attn_insights/sample703_dynamic_attn_alltokens.png (deflated 16%)\n  adding: kaggle/working/attn_insights/sample931_dynamic_attn_tokens.csv (deflated 55%)\n  adding: kaggle/working/attn_insights/sample931_dynamic_attn_prompt.png (deflated 15%)\n  adding: kaggle/working/attn_insights/sample25_dynamic_attn_prompt.png (deflated 14%)\n  adding: kaggle/working/attn_insights/sample98_dynamic_attn_tokens.csv (deflated 58%)\n  adding: kaggle/working/attn_insights/sample25_dynamic_attn_tokens.csv (deflated 57%)\n  adding: kaggle/working/attn_insights/sample790_dynamic_attn_alltokens.png (deflated 18%)\n  adding: kaggle/working/attn_insights/sample790_dynamic_attn_prompt.png (deflated 14%)\n  adding: kaggle/working/attn_insights/sample850_dynamic_attn_alltokens.png (deflated 16%)\n  adding: kaggle/working/attn_insights/sample347_dynamic_attn_alltokens.png (deflated 16%)\n  adding: kaggle/working/attn_insights/sample850_dynamic_attn_prompt.png (deflated 14%)\n  adding: kaggle/working/attn_insights/sample347_dynamic_attn_tokens.csv (deflated 57%)\n  adding: kaggle/working/attn_insights/sample850_dynamic_attn_tokens.csv (deflated 56%)\n  adding: kaggle/working/attn_insights/attention_tail_summary.csv (deflated 34%)\n  adding: kaggle/working/attn_insights/sample324_dynamic_attn_prompt.png (deflated 14%)\n  adding: kaggle/working/attn_insights/sample324_dynamic_attn_tokens.csv (deflated 57%)\n  adding: kaggle/working/attn_insights/sample98_dynamic_attn_alltokens.png (deflated 16%)\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"from datasets import Dataset\n\n# Percorso del file salvato\n# json_path = \"/kaggle/input/final-test-set/final_dataset.json\"\ndataset_path = '/kaggle/input/blurred-dataset/blurred_dataset'\n\n# Carica il dataset Hugging Face\n# hf_dataset = Dataset.from_json(json_path)\nhf_dataset = Dataset.load_from_disk(dataset_path)\n\n# Controlla un esempio\nprint(hf_dataset)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T07:34:14.356069Z","iopub.execute_input":"2025-10-15T07:34:14.356761Z","iopub.status.idle":"2025-10-15T07:34:14.389729Z","shell.execute_reply.started":"2025-10-15T07:34:14.356738Z","shell.execute_reply":"2025-10-15T07:34:14.388983Z"}},"outputs":[{"name":"stdout","text":"Dataset({\n    features: ['img_id', 'image', 'width', 'height', 'label', 'x_t', 'image_k', 'image_k_modified'],\n    num_rows: 1101\n})\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"print(hf_dataset['image_k'][0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T07:34:18.557248Z","iopub.execute_input":"2025-10-15T07:34:18.557529Z","iopub.status.idle":"2025-10-15T07:34:18.566155Z","shell.execute_reply.started":"2025-10-15T07:34:18.557509Z","shell.execute_reply":"2025-10-15T07:34:18.565378Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/images-test/images_test/02a5f39ca584e0d6.jpg\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"from collections import Counter\n\ncounter = Counter(example[\"label\"] for example in hf_dataset)\nprint(counter)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T07:14:42.401805Z","iopub.execute_input":"2025-10-03T07:14:42.402321Z","iopub.status.idle":"2025-10-03T07:14:42.489648Z","shell.execute_reply.started":"2025-10-03T07:14:42.402296Z","shell.execute_reply":"2025-10-03T07:14:42.489098Z"}},"outputs":[{"name":"stdout","text":"Counter({0: 555, 1: 546})\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"from transformers import pipeline\n\n# Carica il classificatore zero-shot\nclassifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n\n# Etichette da prevedere\nimport re\n\nLABELS = [\"real\", \"fake\"]\n\ndef extract_label_from_response(response: str) -> int:\n    result = classifier(response, LABELS)\n    return LABELS.index(result[\"labels\"][0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T11:41:02.151632Z","iopub.execute_input":"2025-10-03T11:41:02.152405Z","iopub.status.idle":"2025-10-03T11:41:22.906702Z","shell.execute_reply.started":"2025-10-03T11:41:02.152380Z","shell.execute_reply":"2025-10-03T11:41:22.905824Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"282340637bc747e4917ba1508b3a9e03"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29ff3e411a0a452c96cebf2fad81cab5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"83d761080d99413e9944a3f1b4ae0177"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3543bb1f183452d8677f23441f65a30"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"08b2f923b0d74de8b74a11c208671073"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21b3ed93ad6e47388aca81817598b710"}},"metadata":{}},{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"!pip install -U peft\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T11:41:35.223413Z","iopub.execute_input":"2025-10-03T11:41:35.224443Z","iopub.status.idle":"2025-10-03T11:41:40.467129Z","shell.execute_reply.started":"2025-10-03T11:41:35.224414Z","shell.execute_reply":"2025-10-03T11:41:40.466254Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.15.2)\nCollecting peft\n  Downloading peft-0.17.1-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from peft) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from peft) (25.0)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (7.0.0)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from peft) (6.0.2)\nRequirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft) (2.6.0+cu124)\nRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from peft) (4.57.0.dev0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from peft) (4.67.1)\nRequirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft) (1.8.1)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from peft) (0.5.3)\nRequirement already satisfied: huggingface_hub>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from peft) (1.0.0rc2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (3.18.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (2025.5.1)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (0.28.1)\nRequirement already satisfied: typer-slim in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (0.19.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (4.14.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (1.1.5)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->peft) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->peft) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->peft) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->peft) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->peft) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->peft) (2.4.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (2.32.4)\nRequirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (0.22.1)\nRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->huggingface_hub>=0.25.0->peft) (4.9.0)\nRequirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->huggingface_hub>=0.25.0->peft) (2025.6.15)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->huggingface_hub>=0.25.0->peft) (1.0.9)\nRequirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->huggingface_hub>=0.25.0->peft) (3.10)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface_hub>=0.25.0->peft) (0.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->peft) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->peft) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->peft) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->peft) (2024.2.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers->peft) (3.4.2)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers->peft) (2.5.0)\nRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer-slim->huggingface_hub>=0.25.0->peft) (8.2.1)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->peft) (2024.2.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->huggingface_hub>=0.25.0->peft) (1.3.1)\nDownloading peft-0.17.1-py3-none-any.whl (504 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m504.9/504.9 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: peft\n  Attempting uninstall: peft\n    Found existing installation: peft 0.15.2\n    Uninstalling peft-0.15.2:\n      Successfully uninstalled peft-0.15.2\nSuccessfully installed peft-0.17.1\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"dynamic_prompt_2 = f\"You are an expert in image forensics. First, carefully analyze the following description of the image, focusing on details that might reveal whether it is generated or real:.\"\ndynamic_prompt_3 = \"\\nAfter your analysis, state your finale judgment: 'REAL' or 'FAKE'.\"\n                                #\"After your analysis, state your finale judgment: 'REAL' or 'FAKE'.\"\nprint(dynamic_prompt_2+dynamic_prompt_3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T11:43:44.328186Z","iopub.execute_input":"2025-10-03T11:43:44.328554Z","iopub.status.idle":"2025-10-03T11:43:44.334083Z","shell.execute_reply.started":"2025-10-03T11:43:44.328529Z","shell.execute_reply":"2025-10-03T11:43:44.333217Z"}},"outputs":[{"name":"stdout","text":"You are an expert in image forensics. First, carefully analyze the following description of the image, focusing on details that might reveal whether it is generated or real:.\nAfter your analysis, state your finale judgment: 'REAL' or 'FAKE'.\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor, BitsAndBytesConfig, GenerationConfig, LogitsProcessorList, MinLengthLogitsProcessor\nfrom PIL import Image\nimport torch\nimport os\n\nos.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n\n\nFEW_SHOT_EXAMPLES = [\n    # Real examples (fotografie autentiche)\n    {\n        \"context\": \"A street photo taken at midday with perfectly natural shadows and realistic reflections on wet pavement.\",\n        \"label\": \"[Real]\",\n        \"motivation\": \"Consistent lighting and natural shadow falloff; reflections sono coerenti con le superfici.\"\n    },\n    {\n        \"context\": \"An outdoor portrait under golden hour light, showing morbidi gradienti di colore nel cielo e dettagli realistici nella pelle.\",\n        \"label\": \"[Real]\",\n        \"motivation\": \"Transizioni tonali naturali e texture dettagliate senza artefatti di sintesi.\"\n    },\n    # Fake examples (AI-generated)\n    {\n        \"context\": \"A foggy forest scene with a faint glow halo around certain trees and pixel-level repeating patterns on the foliage.\",\n        \"label\": \"[Fake]\",\n        \"motivation\": \"The halo suggests blending artifacts; repeating patches betray AI-generation.\"\n    },\n    {\n        \"context\": \"An indoor scene where the textures on the walls appear overly smooth and uniform, and shadows lack realistic variation.\",\n        \"label\": \"[Fake]\",\n        \"motivation\": \"Uniform texture and flat shadows are typical AI generation artifacts.\"\n    }\n]\n\nSTATIC_PROMPTS = [\n    \"Is the image real or fake? Answer with 'REAL' or 'FAKE'.\",\n    \"Analyze and classify: [Real] / [Fake].\",\n    \"Based on lighting, texture, and edges, decide: [Real] / [Fake].\"\n]\n\nMODEL = \"Qwen/Qwen2.5-VL-3B-Instruct\"\n\n# --- mette QUESTO BLOCCO PRIMA di class QwenVLTester(...) ---\n\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\nfrom typing import Dict, List, Tuple\n\nATTN_OUTDIR = \"/kaggle/working/attn_insights\"\nos.makedirs(ATTN_OUTDIR, exist_ok=True)\n\ndef _tokens_from_ids(tokenizer, input_ids: List[int]) -> List[str]:\n    toks = tokenizer.convert_ids_to_tokens(input_ids)\n    return [t.replace(\"Ġ\",\" \").replace(\"▁\",\" \") for t in toks]\n\ndef _mask_prompt_tokens(tokenizer, toks: List[str]) -> List[bool]:\n    specials = set(tokenizer.all_special_tokens)\n    keep = []\n    for t in toks:\n        t_stripped = t.strip()\n        if not t_stripped or t_stripped in specials:\n            keep.append(False); continue\n        if \"<image>\" in t_stripped or \"<img>\" in t_stripped or \"<vision>\" in t_stripped:\n            keep.append(False); continue\n        keep.append(True)\n    return keep\n\ndef _aggregate_attention(attentions: List[torch.Tensor], layer_reduce: str = \"mean\") -> torch.Tensor:\n    \"\"\"\n    attentions: lista di per-layer (batch, n_heads, seq, seq) oppure None.\n    Ritorna (batch, seq, seq) mediando sulle heads e combinando i layer non-None.\n    \"\"\"\n    valid_layers = [a for a in attentions if a is not None]\n    if len(valid_layers) == 0:\n        raise ValueError(\"Nessun layer di attention disponibile (tutti None). \"\n                         \"Assicurati che output_attentions=True e use_cache=False nel forward.\")\n    # media su heads layer-per-layer\n    per_layer = [a.mean(dim=1) for a in valid_layers]  # (batch, seq, seq)\n    if layer_reduce == \"mean\":\n        A = torch.stack(per_layer, dim=0).mean(dim=0)  # media su layer validi\n    elif layer_reduce == \"sum\":\n        A = torch.stack(per_layer, dim=0).sum(dim=0)   # somma su layer validi\n    else:\n        A = per_layer[-1]  # ultimo layer valido\n    return A  # (batch, seq, seq)\n\n\ndef _normalize(v: np.ndarray) -> np.ndarray:\n    v = np.asarray(v, dtype=float); s = v.sum()\n    return v / s if s > 0 else v\n\nclass AttentionInspectorMixin:\n    def forward_with_attn(self, image_path: str, prompt: str):\n        img = Image.open(image_path).convert(\"RGB\").resize((224, 224), Image.BILINEAR)\n        chat = self.processor.apply_chat_template(\n            [{\"role\": \"user\", \"content\": [{\"type\": \"image\", \"image\": None}, {\"type\": \"text\", \"text\": prompt}]}],\n            tokenize=False, add_generation_prompt=True\n        )\n        inputs = self.processor(text=[chat], images=[img], padding=True, return_tensors=\"pt\").to(self.model.device)\n        with torch.no_grad():\n            out = self.model(**inputs, output_attentions=True, return_dict=True)\n        return inputs, out\n\n    def analyze_prompt_attention(self, image_path: str, prompt: str, file_stem: str,\n                                 layer_reduce: str = \"mean\", save_with_image_tokens: bool = False) -> Dict[str, str]:\n        inputs, out = self.forward_with_attn(image_path, prompt)\n        assert out.attentions is not None, \"output_attentions non fornito dal modello.\"\n        input_ids = inputs[\"input_ids\"][0].tolist()\n        toks = _tokens_from_ids(self.processor.tokenizer, input_ids)\n        keep_mask = _mask_prompt_tokens(self.processor.tokenizer, toks)\n        A = _aggregate_attention(out.attentions, layer_reduce=layer_reduce)[0].float().cpu().numpy()\n        q_idx = inputs[\"input_ids\"].shape[-1] - 1\n        attn_to_all = _normalize(A[q_idx])\n\n        idx_prompt = [i for i, k in enumerate(keep_mask) if k]\n        toks_prompt = [toks[i] for i in idx_prompt]\n        scores_prompt = attn_to_all[idx_prompt]\n        order = np.argsort(scores_prompt)[::-1]\n        toks_sorted = [toks_prompt[i] for i in order]\n        idx_sorted  = [idx_prompt[i]  for i in order]\n        scr_sorted  = [float(scores_prompt[i]) for i in order]\n        cum_sorted  = list(np.cumsum(scr_sorted))\n\n        # CSV ranking\n        csv_path = os.path.join(ATTN_OUTDIR, f\"{file_stem}_attn_tokens.csv\")\n        import csv\n        with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n            w = csv.writer(f)\n            w.writerow([\"rank\",\"token\",\"input_index\",\"score\",\"cum_score\"])\n            for r,(tk,ii,ss,cc) in enumerate(zip(toks_sorted, idx_sorted, scr_sorted, cum_sorted), start=1):\n                w.writerow([r, tk, ii, f\"{ss:.6f}\", f\"{cc:.6f}\"])\n\n        # Heatmap prompt\n        fig = plt.figure(figsize=(max(8, len(toks_prompt)*0.25), 2.5))\n        plt.imshow(scores_prompt[np.newaxis, :], aspect=\"auto\")\n        plt.yticks([0], [\"attention score\"])\n        plt.xticks(range(len(toks_prompt)), toks_prompt, rotation=80, ha=\"right\")\n        plt.title(\"Attention to prompt tokens (last prompt token as query)\")\n        plt.colorbar(); plt.tight_layout()\n        png_prompt = os.path.join(ATTN_OUTDIR, f\"{file_stem}_attn_prompt.png\")\n        fig.savefig(png_prompt, dpi=220); plt.close(fig)\n\n        png_full = \"\"\n        if save_with_image_tokens:\n            fig2 = plt.figure(figsize=(max(8, len(toks)*0.22), 2.8))\n            plt.imshow(attn_to_all[np.newaxis, :], aspect=\"auto\")\n            plt.yticks([0], [\"attention score\"])\n            plt.xticks(range(len(toks)), toks, rotation=80, ha=\"right\")\n            plt.title(\"Attention to ALL input tokens (incl. image markers)\")\n            plt.colorbar(); plt.tight_layout()\n            png_full = os.path.join(ATTN_OUTDIR, f\"{file_stem}_attn_alltokens.png\")\n            fig2.savefig(png_full, dpi=220); plt.close(fig2)\n\n        return {\"csv\": csv_path, \"png_prompt\": png_prompt, \"png_alltokens\": png_full}\n\n\nclass QwenVLTester(AttentionInspectorMixin):\n    def __init__(self):\n        super().__init__()\n        self.model_id = \"/kaggle/input/finetuned-model-2/finetuned_model\"\n        # Configurazione per 4-bit quantization\n        bnb_config = BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_use_double_quant=True,\n            bnb_4bit_quant_type=\"nf4\",\n            bnb_4bit_compute_dtype=torch.float16\n        )\n\n        # Caricamento modello e processor\n        #self.model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n            #self.model_id,\n            #quantization_config=bnb_config,\n            #device_map=\"auto\"\n        #)\n\n        from peft import PeftModel\n\n        # Carica il modello base con attenzione \"eager\"\n        base_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n            MODEL,\n            quantization_config=bnb_config,\n            device_map=\"auto\",\n            attn_implementation=\"eager\",      # <--- importante\n        )\n        \n        # Applica i pesi LoRA fine-tunati\n        self.model = PeftModel.from_pretrained(base_model, self.model_id)\n\n        self.model.config.output_attentions = True\n        self.model.eval()\n\n\n        self.processor = AutoProcessor.from_pretrained(\n            MODEL,\n            image_processor_kwargs={\"size\": (224, 224)}\n        )\n\n        self.gen_config = GenerationConfig(\n            do_sample=True,\n            temperature=0.7,\n            top_p=0.9,\n            max_new_tokens=100\n        )\n\n\n    def build_prompt(self, prompt_type: str = \"dynamic\", x_t: str = \"\") -> str:\n        # Costruisce un prompt few-shot con esempi rinforzati\n        few_shot = \"\".join([\n            f\"Example:\\nContext: {ex['context']}\\nAnswer: {ex['label']} Motivation: {ex['motivation']}\\n\\n\"\n            for ex in FEW_SHOT_EXAMPLES\n        ])\n\n        if prompt_type == \"dynamic\" and x_t:\n            # Context dinamico fornito dall'utente\n            dynamic_prompt = (\n                \"You are an expert in image forensics. \"\n                \"First, carefully analyze the following description of the image, focusing on details that might reveal whether it is generated or real:\\n\"\n                \"[TECH_START]\\n\"\n                f\"{x_t.strip()}\\n\"\n                \"[TECH_END]\\n\"\n                \"After your analysis, state your final judgment: 'REAL' or 'FAKE'.\"\n            )\n            return dynamic_prompt\n        else:\n            # Prompt statico, con uno dei template\n            template = STATIC_PROMPTS[int(prompt_type.split('_')[-1]) if '_' in prompt_type else 0]\n            return (\n                f\"Question: {template}\\nAnswer:\"  # obbliga scelta esplicita\n            )\n\n    def generate(self, image_path: str, prompt: str) -> str:\n        # 1) Carica immagine\n        img = Image.open(image_path).convert(\"RGB\").resize((224, 224), Image.BILINEAR)\n\n        # 2) Applica template chat\n        chat = self.processor.apply_chat_template(\n            [{\"role\": \"user\", \"content\": [{\"type\": \"image\", \"image\": None}, {\"type\": \"text\", \"text\": prompt}]}],\n            tokenize=False,\n            add_generation_prompt=True\n        )\n\n        # 3) Tokenizzazione + immagini\n        inputs = self.processor(\n            text=[chat],\n            images=[img],\n            padding=True,\n            return_tensors=\"pt\"\n        ).to(self.model.device)\n\n        # 4) Generazione con sampling leggero e top-p\n        out = self.model.generate(\n            **inputs,\n            generation_config=self.gen_config,\n            return_dict_in_generate=True,\n            use_cache=False,\n            output_scores=True\n        )\n\n        # 5) Decodifica\n        seq = out.sequences[:, inputs[\"input_ids\"].shape[-1]:]\n        answer = self.processor.batch_decode(\n            seq,\n            skip_special_tokens=True,\n            clean_up_tokenization_spaces=True\n        )\n        return answer[0].strip()\n\n    def extract_label(self, response: str) -> int:\n        # Estrae la label numerica a partire dalla risposta testuale\n        return extract_label_from_response(response)\n\n    def test(self, example: dict, prompt_type: str = \"dynamic\"):\n        # Interfaccia principale per testare un esempio\n        torch.cuda.empty_cache()\n        prompt = self.build_prompt(prompt_type=prompt_type, x_t=example.get(\"x_t\", \"\"))\n        response = self.generate(example[\"image_k\"], prompt)\n        label = self.extract_label(response)\n        return label, response\n\n# Funzione wrapper\n_tester_instance = QwenVLTester()\n\ndef test_qwen_vl(example, prompt_type=\"dynamic\"):\n    return _tester_instance.test(example, prompt_type)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T11:54:39.846790Z","iopub.execute_input":"2025-10-03T11:54:39.847631Z","iopub.status.idle":"2025-10-03T11:54:57.905048Z","shell.execute_reply.started":"2025-10-03T11:54:39.847590Z","shell.execute_reply":"2025-10-03T11:54:57.904103Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a9ae1efa1054756aeae1843db963a4a"}},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"# ================== CONFIG ==================\nWINDOW_LAST_N = 30\nMAX_SAMPLES   = 2     # quanti esempi analizzare (a piacere)\nOUT_DIR       = \"/kaggle/working/attn_insights_new_prompt\"\n# ============================================\n\nimport os\nimport math\nimport csv\nimport numpy as np\nimport pandas as pd\nos.makedirs(OUT_DIR, exist_ok=True)\n\ndef _char_to_token_idx_via_greedy(toks: list[str], char_pos: int) -> int:\n    \"\"\"\n    Grezza ma efficace: costruisce cumulata delle lunghezze dei token grezzi\n    (così come convert_ids_to_tokens li restituisce) e trova il primo token\n    la cui cumulata supera char_pos.\n    \"\"\"\n    cum = []\n    s = \"\"\n    for t in toks:\n        s += t\n        cum.append(len(s))\n    for i, c in enumerate(cum):\n        if c >= char_pos:\n            return i\n    return len(toks) - 1\n\ndef _segment_token_span_from_prompt_text(prompt: str, toks_all: list[str]) -> tuple[int, int] | None:\n    # 1) prova con i nuovi tag\n    start_char = prompt.find(\"[TECH_START]\")\n    end_char   = prompt.find(\"[TECH_END]\")\n    if start_char != -1 and end_char != -1 and end_char > start_char:\n        # il contenuto tecnico è tra i tag, escludendo i tag stessi\n        start_char = start_char + len(\"[TECH_START]\")\n        # end_char rimane l'inizio di [TECH_END]\n        start_tok = _char_to_token_idx_via_greedy(toks_all, start_char)\n        end_tok   = _char_to_token_idx_via_greedy(toks_all, end_char)\n        return (start_tok, max(start_tok, end_tok-1))\n\n    # 2) fallback legacy: vecchi marcatori (se mai tornassero)\n    start_str = \"Image Description and Technical Analysis:\"\n    end_str   = \"Question:\"\n    start_char = prompt.find(start_str)\n    end_char   = prompt.find(end_str)\n    if start_char != -1 and end_char != -1 and end_char > start_char:\n        start_tok = _char_to_token_idx_via_greedy(toks_all, start_char + len(start_str))\n        end_tok   = _char_to_token_idx_via_greedy(toks_all, end_char)\n        return (start_tok, max(start_tok, end_tok-1))\n\n    # 3) fallback minimal: prova a trovare direttamente x_t (se lo passi alla funzione o lo hai disponibile)\n    return None\n\ndef _attention_share_last_n(scores_prompt: np.ndarray, n_last: int = 30) -> dict:\n    n = len(scores_prompt)\n    n_tail = min(n_last, n)\n    mass_last_n = float(scores_prompt[-n_tail:].sum()) if n > 0 else float(\"nan\")\n    return {\"mass_last_n\": mass_last_n, \"n_prompt_tokens\": n, \"n_tail\": n_tail}\n\ndef _attention_share_technical(attn_to_all: np.ndarray,\n                               idx_prompt: list[int],\n                               tech_span_all_tokens: tuple[int, int] | None) -> float | float:\n    \"\"\"\n    Calcola la quota di attenzione riservata ai token del SEGMENTO TECNICO,\n    limitandosi ai soli token del PROMPT (esclude special/immagine).\n    - attn_to_all: vettore attenzione normalizzato (len = seq_len totale)\n    - idx_prompt: indici dei token di prompt “tenuti” (solo testo prompt)\n    - tech_span_all_tokens: (start,end) sugli indici dell'intera sequenza (prima del filtro)\n    \"\"\"\n    if tech_span_all_tokens is None:\n        return float(\"nan\")\n    start_all, end_all = tech_span_all_tokens\n    end_all = max(start_all, end_all)\n    # prendi gli indici di prompt che cadono nel range tecnico in spazi token globali\n    tech_prompt_positions = [j for j in idx_prompt if start_all <= j <= end_all]\n    if len(tech_prompt_positions) == 0:\n        return float(\"nan\")\n    return float(attn_to_all[tech_prompt_positions].sum())\n\ndef analyze_subset_with_technical(dataset,\n                                  tester,\n                                  idxs,\n                                  prompt_mode=\"dynamic\",\n                                  window_last_n: int = 30) -> pd.DataFrame:\n    \"\"\"\n    Per ciascun esempio:\n      - calcola mass_last_n sugli ultimi N token\n      - se dynamic: calcola mass_technical sul blocco x_t\n      - ritorna DataFrame riga-per-esempio\n    \"\"\"\n    rows = []\n    for i in idxs:\n        ex = dataset[i]\n        prompt = tester.build_prompt(prompt_type=prompt_mode, x_t=ex.get(\"x_t\",\"\"))\n        # Forward solo per attention (no generazione)\n        inputs, out = tester.forward_with_attn(ex[\"image_k\"], prompt)\n\n        input_ids = inputs[\"input_ids\"][0].tolist()\n        toks_all  = _tokens_from_ids(tester.processor.tokenizer, input_ids)\n        keep_mask = _mask_prompt_tokens(tester.processor.tokenizer, toks_all)\n\n        # aggrega heads/layers e prendi attn dell'ultimo token del prompt (come query)\n        A = _aggregate_attention(out.attentions, \"mean\")[0].float().cpu().numpy()\n        q_idx = inputs[\"input_ids\"].shape[-1] - 1\n        attn_to_all = _normalize(A[q_idx])\n\n        idx_prompt = [j for j, k in enumerate(keep_mask) if k]\n        scores_prompt = attn_to_all[idx_prompt]\n\n        # quota ultimi N token\n        share_tail = _attention_share_last_n(scores_prompt, n_last=window_last_n)\n\n        # quota blocco tecnico (solo per dynamic)\n        if prompt_mode.startswith(\"dynamic\"):\n            tech_span = _segment_token_span_from_prompt_text(prompt, toks_all)\n            mass_technical = _attention_share_technical(attn_to_all, idx_prompt, tech_span)\n        else:\n            mass_technical = float(\"nan\")\n\n        rows.append({\n            \"idx\": i,\n            \"prompt_mode\": prompt_mode,\n            \"mass_last_n\": share_tail[\"mass_last_n\"],\n            \"n_prompt_tokens\": share_tail[\"n_prompt_tokens\"],\n            \"n_tail\": share_tail[\"n_tail\"],\n            \"mass_technical\": mass_technical\n        })\n    return pd.DataFrame(rows)\n\n# ----- ESECUZIONE: dynamic + static -----\nN = min(MAX_SAMPLES, len(hf_dataset))\nidxs = list(range(N))\n\ndf_dyn = analyze_subset_with_technical(hf_dataset, _tester_instance, idxs, prompt_mode=\"dynamic\",  window_last_n=WINDOW_LAST_N)\ndf_sta = analyze_subset_with_technical(hf_dataset, _tester_instance, idxs, prompt_mode=\"static_0\", window_last_n=WINDOW_LAST_N)\n\ndf_all = pd.concat([df_dyn, df_sta], ignore_index=True)\n\n# salva dettagli per esempio\nper_example_csv = os.path.join(OUT_DIR, \"attention_shares_with_technical.csv\")\ndf_all.to_csv(per_example_csv, index=False)\nprint(\"[OK] Per-example ->\", per_example_csv)\n\n# riepilogo per modalità\nsummary = (df_all.groupby(\"prompt_mode\")[[\"mass_last_n\",\"mass_technical\"]]\n                  .agg([\"mean\",\"std\",\"median\",\"min\",\"max\"]))\nsummary_csv = os.path.join(OUT_DIR, \"attention_shares_with_technical_summary.csv\")\nsummary.to_csv(summary_csv)\nprint(\"[OK] Summary ->\", summary_csv)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T11:55:48.848835Z","iopub.execute_input":"2025-10-03T11:55:48.849415Z","iopub.status.idle":"2025-10-03T11:55:50.794537Z","shell.execute_reply.started":"2025-10-03T11:55:48.849392Z","shell.execute_reply":"2025-10-03T11:55:50.793829Z"}},"outputs":[{"name":"stdout","text":"[OK] Per-example -> /kaggle/working/attn_insights_new_prompt/attention_shares_with_technical.csv\n[OK] Summary -> /kaggle/working/attn_insights_new_prompt/attention_shares_with_technical_summary.csv\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n  has_large_values = (abs_vals > 1e6).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"            mass_last_n                                          \\\n                   mean       std    median       min       max   \nprompt_mode                                                       \ndynamic        0.444410  0.002185  0.444410  0.442865  0.445955   \nstatic_0       0.563659  0.002646  0.563659  0.561788  0.565530   \n\n            mass_technical                                          \n                      mean       std    median       min       max  \nprompt_mode                                                         \ndynamic           0.108772  0.002588  0.108772  0.106942  0.110602  \nstatic_0               NaN       NaN       NaN       NaN       NaN  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead tr th {\n        text-align: left;\n    }\n\n    .dataframe thead tr:last-of-type th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr>\n      <th></th>\n      <th colspan=\"5\" halign=\"left\">mass_last_n</th>\n      <th colspan=\"5\" halign=\"left\">mass_technical</th>\n    </tr>\n    <tr>\n      <th></th>\n      <th>mean</th>\n      <th>std</th>\n      <th>median</th>\n      <th>min</th>\n      <th>max</th>\n      <th>mean</th>\n      <th>std</th>\n      <th>median</th>\n      <th>min</th>\n      <th>max</th>\n    </tr>\n    <tr>\n      <th>prompt_mode</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>dynamic</th>\n      <td>0.444410</td>\n      <td>0.002185</td>\n      <td>0.444410</td>\n      <td>0.442865</td>\n      <td>0.445955</td>\n      <td>0.108772</td>\n      <td>0.002588</td>\n      <td>0.108772</td>\n      <td>0.106942</td>\n      <td>0.110602</td>\n    </tr>\n    <tr>\n      <th>static_0</th>\n      <td>0.563659</td>\n      <td>0.002646</td>\n      <td>0.563659</td>\n      <td>0.561788</td>\n      <td>0.565530</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"\nimport os\nfrom typing import List, Dict\n\ndef generate_heatmaps_for_indices(\n    dataset,\n    tester,\n    indices,\n    prompt_mode=\"dynamic\",\n    out_dir=\"/kaggle/working/attn_insights\",\n    save_with_image_tokens=True\n):\n    import os\n    os.makedirs(out_dir, exist_ok=True)\n\n    # cast a int Python e (facoltativo) filtra out-of-range\n    indices = [int(i) for i in indices if 0 <= int(i) < len(dataset)]\n\n    results = []\n    for idx in indices:\n        ex = dataset[idx]  # ora è int Python\n        prompt = tester.build_prompt(prompt_type=prompt_mode, x_t=ex.get(\"x_t\", \"\"))\n        stem = f\"sample{idx}_{prompt_mode}\"\n        paths = tester.analyze_prompt_attention(\n            image_path=ex[\"image_k\"],\n            prompt=prompt,\n            file_stem=stem,\n            layer_reduce=\"mean\",\n            save_with_image_tokens=save_with_image_tokens\n        )\n        paths.update({\"idx\": idx, \"prompt_mode\": prompt_mode})\n        results.append(paths)\n        print(f\"[OK] {stem} -> {paths['png_prompt']} | {paths['csv']}\" + (f\" | {paths['png_alltokens']}\" if paths['png_alltokens'] else \"\"))\n    return results\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T11:57:37.169133Z","iopub.execute_input":"2025-10-03T11:57:37.169455Z","iopub.status.idle":"2025-10-03T11:57:37.177106Z","shell.execute_reply.started":"2025-10-03T11:57:37.169434Z","shell.execute_reply":"2025-10-03T11:57:37.176032Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\ndef pick_examples_by_outcome(\n    preds_csv: str,\n    k_per_bin: int = 2,\n    seed: int = 0,\n    idx_col: str | None = \"idx\",          # se None o non presente, usa df.index\n    gt_col: str = \"gt_label\",\n    pred_col: str = \"pred_label\"\n):\n    \"\"\"\n    Restituisce indici per TP/TN/FP/FN.\n    Se idx_col non esiste, usa l'indice del DataFrame (df.index) come idx.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    df = pd.read_csv(preds_csv)\n\n    # determina colonna idx\n    if idx_col is None or idx_col not in df.columns:\n        df[\"__idx__\"] = df.index\n        idx_col = \"__idx__\"\n\n    # normalizza stringhe\n    def _norm(v):\n        if isinstance(v, str): return v.strip().upper()\n        return v\n\n    if gt_col not in df.columns or pred_col not in df.columns:\n        raise ValueError(f\"Mancano colonne {gt_col}/{pred_col} nel CSV. Colonne disponibili: {list(df.columns)}\")\n\n    df[\"gt_n\"]   = df[gt_col].map(_norm)\n    df[\"pred_n\"] = df[pred_col].map(_norm)\n\n    # mappa a 0/1 (gestisce stringhe/booleani)\n    def _to01(v):\n        if v in (\"REAL\", 1, \"1\", \"TRUE\", True):  return 1\n        if v in (\"FAKE\", 0, \"0\", \"FALSE\", False): return 0\n        return np.nan\n\n    df[\"gt01\"]   = df[\"gt_n\"].map(_to01)\n    df[\"pred01\"] = df[\"pred_n\"].map(_to01)\n    df = df.dropna(subset=[\"gt01\",\"pred01\"])\n\n    df[\"ok\"] = (df[\"gt01\"] == df[\"pred01\"]).astype(int)\n\n    TP = df[(df[\"gt01\"]==1) & (df[\"pred01\"]==1)]\n    TN = df[(df[\"gt01\"]==0) & (df[\"pred01\"]==0)]\n    FP = df[(df[\"gt01\"]==0) & (df[\"pred01\"]==1)]\n    FN = df[(df[\"gt01\"]==1) & (df[\"pred01\"]==0)]\n\n    def pick(dfbin):\n        if len(dfbin) == 0: return []\n        idxs = dfbin[idx_col].tolist()\n        if len(idxs) <= k_per_bin: return idxs\n        return list(rng.choice(idxs, size=k_per_bin, replace=False))\n\n    return {\"TP\": pick(TP), \"TN\": pick(TN), \"FP\": pick(FP), \"FN\": pick(FN)}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T11:57:52.105018Z","iopub.execute_input":"2025-10-03T11:57:52.105509Z","iopub.status.idle":"2025-10-03T11:57:52.115938Z","shell.execute_reply.started":"2025-10-03T11:57:52.105485Z","shell.execute_reply":"2025-10-03T11:57:52.115135Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"bins = pick_examples_by_outcome(\n    \"/kaggle/input/dynamic-finetune/dynamic_test_Qwen_finetuned_2.csv\",\n    k_per_bin=1,\n    idx_col=None,             # <-- usa l'indice del CSV come idx\n    gt_col=\"gt_label\",\n    pred_col=\"pred_label\"\n)\nfor label, idxs in bins.items():\n    print(label, idxs)\n    generate_heatmaps_for_indices(hf_dataset, _tester_instance, idxs, prompt_mode=\"dynamic\", save_with_image_tokens=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T11:57:58.738709Z","iopub.execute_input":"2025-10-03T11:57:58.739472Z","iopub.status.idle":"2025-10-03T11:58:20.529593Z","shell.execute_reply.started":"2025-10-03T11:57:58.739449Z","shell.execute_reply":"2025-10-03T11:58:20.528799Z"}},"outputs":[{"name":"stdout","text":"TP [937]\n[OK] sample937_dynamic -> /kaggle/working/attn_insights/sample937_dynamic_attn_prompt.png | /kaggle/working/attn_insights/sample937_dynamic_attn_tokens.csv | /kaggle/working/attn_insights/sample937_dynamic_attn_alltokens.png\nTN [661]\n[OK] sample661_dynamic -> /kaggle/working/attn_insights/sample661_dynamic_attn_prompt.png | /kaggle/working/attn_insights/sample661_dynamic_attn_tokens.csv | /kaggle/working/attn_insights/sample661_dynamic_attn_alltokens.png\nFP [539]\n[OK] sample539_dynamic -> /kaggle/working/attn_insights/sample539_dynamic_attn_prompt.png | /kaggle/working/attn_insights/sample539_dynamic_attn_tokens.csv | /kaggle/working/attn_insights/sample539_dynamic_attn_alltokens.png\nFN [170]\n[OK] sample170_dynamic -> /kaggle/working/attn_insights/sample170_dynamic_attn_prompt.png | /kaggle/working/attn_insights/sample170_dynamic_attn_tokens.csv | /kaggle/working/attn_insights/sample170_dynamic_attn_alltokens.png\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"from datasets import load_from_disk\nfrom tqdm import tqdm\nimport pandas as pd\n\nDATASET_PATH = \"/kaggle/input/test-dataset-kaggle/test_dataset_kaggle\"\nOUTPUT_PATH = \"./results.csv\"\n\ndef run_test():\n    #dataset = load_from_disk(DATASET_PATH)\n    for i in range(10):\n        #print(\"Dataset structure:\\n\", hf_dataset)\n        example = hf_dataset[i]\n        print(\"Ground_Truth:\\n\", example['label'])\n        #print(\"Image:\\n\", example['image'])\n        #print(\"Prompt:\\n\", example['x_t'])\n        label, response = test_qwen_vl(example, \"static_0\")\n        print(\"Output label:\\n\", label)\n        print(\"Output response:\\n\", response)\n\n#run_test()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-23T12:15:41.912489Z","iopub.execute_input":"2025-09-23T12:15:41.912764Z","iopub.status.idle":"2025-09-23T12:15:54.347581Z","shell.execute_reply.started":"2025-09-23T12:15:41.912743Z","shell.execute_reply":"2025-09-23T12:15:54.346703Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport pandas as pd\nfrom tqdm.auto import tqdm\nfrom datasets import load_from_disk\n\n# DATASET_PATH   = \"/kaggle/input/test-dataset-kaggle/test_dataset_kaggle\"\nCHECKPOINT_CSV = \"/kaggle/working/dynamic_test_Qwen_finetuned_2.csv\"\nPROMPT_TYPE    = \"dynamic\"\nCHECKPOINT_EVERY = 30  # salva ogni 30 campioni\n\n# 1) Carica dataset\n#dataset = load_from_disk(DATASET_PATH)\n\n# 2) Se esiste già un CSV di checkpoint, riloadalo e salta i campioni già processati\nif os.path.exists(CHECKPOINT_CSV):\n    df = pd.read_csv(CHECKPOINT_CSV)\n    processed_ids = set(df[\"img_id\"])\nelse:\n    df = pd.DataFrame(columns=[\"img_id\",\"gt_label\",\"pred_label\",\"response\"])\n    processed_ids = set()\n\n# 3) Loop con tqdm e checkpoint ogni N campioni\nfor sample in tqdm(hf_dataset, desc=f\"Eval {PROMPT_TYPE}\", dynamic_ncols=True):\n    img_id = sample[\"img_id\"]\n    if img_id in processed_ids:\n        continue\n\n    try:\n        pred_label, resp = test_qwen_vl(sample, prompt_type=PROMPT_TYPE)\n    except Exception as e:\n        print(\"Errore nella generazione\")\n        pred_label, resp = -1, f\"[ERROR] {e}\"\n\n    # prepara la nuova riga\n    row = {\n        \"img_id\":     img_id,\n        \"gt_label\":   sample[\"label\"],\n        \"pred_label\": pred_label,\n        \"response\":   resp\n    }\n    # concatena in un colpo solo\n    df = pd.concat([df, pd.DataFrame([row])], ignore_index=True)\n    processed_ids.add(img_id)\n\n    # salva checkpoint\n    if len(processed_ids) % CHECKPOINT_EVERY == 0:\n        df.to_csv(CHECKPOINT_CSV, index=False)\n\n# 4) Alla fine salva il CSV definitivo\ndf.to_csv(CHECKPOINT_CSV, index=False)\nprint(\"✅ Checkpoint salvato in:\", CHECKPOINT_CSV)\n\n# 5) Calcolo accuracy su quelli validi\nvalid = df[\"pred_label\"] != -1\nacc = (df.loc[valid, \"gt_label\"] == df.loc[valid, \"pred_label\"]).mean()\nprint(f\"Accuracy {PROMPT_TYPE}: {acc:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T11:49:00.544317Z","iopub.execute_input":"2025-09-22T11:49:00.544841Z","execution_failed":"2025-09-22T12:09:36.666Z"}},"outputs":[],"execution_count":null}]}