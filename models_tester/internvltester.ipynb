{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12457166,"sourceType":"datasetVersion","datasetId":7858089},{"sourceId":12457216,"sourceType":"datasetVersion","datasetId":7858132},{"sourceId":12657249,"sourceType":"datasetVersion","datasetId":7998846},{"sourceId":12657280,"sourceType":"datasetVersion","datasetId":7998865},{"sourceId":12658472,"sourceType":"datasetVersion","datasetId":7999569},{"sourceId":12924296,"sourceType":"datasetVersion","datasetId":8178199},{"sourceId":12928267,"sourceType":"datasetVersion","datasetId":8180731},{"sourceId":12943779,"sourceType":"datasetVersion","datasetId":8191200}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install git+https://github.com/huggingface/transformers accelerate qwen-vl-utils bitsandbytes","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-17T06:20:06.557724Z","iopub.execute_input":"2025-07-17T06:20:06.557939Z","iopub.status.idle":"2025-07-17T06:21:52.564100Z","shell.execute_reply.started":"2025-07-17T06:20:06.557921Z","shell.execute_reply":"2025-07-17T06:21:52.563134Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T06:22:16.313361Z","iopub.execute_input":"2025-07-17T06:22:16.314034Z","iopub.status.idle":"2025-07-17T06:22:19.511421Z","shell.execute_reply.started":"2025-07-17T06:22:16.314000Z","shell.execute_reply":"2025-07-17T06:22:19.510689Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import Dataset\n\n# Percorso del file salvato\n# json_path = \"/kaggle/input/final-test-set/final_dataset.json\"\ndataset_path = '/kaggle/input/summarized-dataset/dataset_summarized'\n\n# Carica il dataset Hugging Face\n# hf_dataset = Dataset.from_json(json_path)\nhf_dataset = Dataset.load_from_disk(dataset_path)\n\n# Controlla un esempio\nprint(hf_dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T06:22:37.199349Z","iopub.execute_input":"2025-07-17T06:22:37.200076Z","iopub.status.idle":"2025-07-17T06:22:38.931562Z","shell.execute_reply.started":"2025-07-17T06:22:37.200029Z","shell.execute_reply":"2025-07-17T06:22:38.930782Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import pipeline\n\n# Carica il classificatore zero-shot\nclassifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n\n# Etichette da prevedere\nimport re\n\nLABELS = [\"real\", \"fake\"]\n\ndef extract_label_from_response(response: str) -> int:\n    result = classifier(response, LABELS)\n    return LABELS.index(result[\"labels\"][0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T06:22:42.265282Z","iopub.execute_input":"2025-07-17T06:22:42.265683Z","iopub.status.idle":"2025-07-17T06:23:15.294425Z","shell.execute_reply.started":"2025-07-17T06:22:42.265662Z","shell.execute_reply":"2025-07-17T06:23:15.293780Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torchvision.transforms as T\nfrom PIL import Image\nfrom torchvision.transforms.functional import InterpolationMode\nfrom transformers import AutoModel, AutoTokenizer\n\nIMAGENET_MEAN = (0.485, 0.456, 0.406)\nIMAGENET_STD = (0.229, 0.224, 0.225)\n\ndef build_transform(input_size):\n    MEAN, STD = IMAGENET_MEAN, IMAGENET_STD\n    transform = T.Compose([\n        T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n        T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),\n        T.ToTensor(),\n        T.Normalize(mean=MEAN, std=STD)\n    ])\n    return transform\n\ndef find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):\n    best_ratio_diff = float('inf')\n    best_ratio = (1, 1)\n    area = width * height\n    for ratio in target_ratios:\n        target_aspect_ratio = ratio[0] / ratio[1]\n        ratio_diff = abs(aspect_ratio - target_aspect_ratio)\n        if ratio_diff < best_ratio_diff:\n            best_ratio_diff = ratio_diff\n            best_ratio = ratio\n        elif ratio_diff == best_ratio_diff:\n            if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:\n                best_ratio = ratio\n    return best_ratio\n\ndef dynamic_preprocess(image, min_num=1, max_num=12, image_size=448, use_thumbnail=False):\n    orig_width, orig_height = image.size\n    aspect_ratio = orig_width / orig_height\n\n    # calculate the existing image aspect ratio\n    target_ratios = set(\n        (i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if\n        i * j <= max_num and i * j >= min_num)\n    target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])\n\n    # find the closest aspect ratio to the target\n    target_aspect_ratio = find_closest_aspect_ratio(\n        aspect_ratio, target_ratios, orig_width, orig_height, image_size)\n\n    # calculate the target width and height\n    target_width = image_size * target_aspect_ratio[0]\n    target_height = image_size * target_aspect_ratio[1]\n    blocks = target_aspect_ratio[0] * target_aspect_ratio[1]\n\n    # resize the image\n    resized_img = image.resize((target_width, target_height))\n    processed_images = []\n    for i in range(blocks):\n        box = (\n            (i % (target_width // image_size)) * image_size,\n            (i // (target_width // image_size)) * image_size,\n            ((i % (target_width // image_size)) + 1) * image_size,\n            ((i // (target_width // image_size)) + 1) * image_size\n        )\n        # split the image\n        split_img = resized_img.crop(box)\n        processed_images.append(split_img)\n    assert len(processed_images) == blocks\n    if use_thumbnail and len(processed_images) != 1:\n        thumbnail_img = image.resize((image_size, image_size))\n        processed_images.append(thumbnail_img)\n    return processed_images\n\ndef load_image(image_file, input_size=448, max_num=12):\n    image = Image.open(image_file).convert('RGB')\n    transform = build_transform(input_size=input_size)\n    images = dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=max_num)\n    pixel_values = [transform(image) for image in images]\n    pixel_values = torch.stack(pixel_values)\n    return pixel_values","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T06:24:11.750963Z","iopub.execute_input":"2025-07-17T06:24:11.752397Z","iopub.status.idle":"2025-07-17T06:24:11.768577Z","shell.execute_reply.started":"2025-07-17T06:24:11.752359Z","shell.execute_reply":"2025-07-17T06:24:11.767866Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\nos.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n\n# Few-shot esempi per real/fake\nFEW_SHOT_EXAMPLES = [\n    {\n        \"context\": \"A street photo taken at midday with perfectly natural shadows and realistic reflections on wet pavement.\",\n        \"label\": \"[Real]\",\n        \"motivation\": \"Consistent lighting and natural shadow falloff; reflections are coherent with the surfaces.\"\n    },\n    {\n        \"context\": \"An indoor scene where the textures on the walls appear overly smooth and uniform, and shadows lack realistic variation.\",\n        \"label\": \"[Fake]\",\n        \"motivation\": \"Uniform texture and flat shadows are typical AI generation artifacts.\"\n    },\n]\n\nSTATIC_PROMPTS = [\n    \"<image>\\nIs this image real or fake? Choose one: [Real] / [Fake].\",\n    \"<image>\\nAnalyze and classify: [Real] / [Fake].\",\n    \"<image>\\nBased on lighting and texture, decide: [Real] / [Fake].\"\n]\n\nclass InternVLTester:\n    def __init__(self, model_id=\"OpenGVLab/InternVL2_5-4B\", device=\"cuda:0\"):\n        self.device = torch.device(device)\n        self.model = AutoModel.from_pretrained(\n            model_id,\n            torch_dtype=torch.bfloat16,\n            load_in_8bit=True,\n            low_cpu_mem_usage=True,\n            trust_remote_code=True).eval()\n        \n        self.tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True, use_fast=False)\n\n    def build_prompt(self, prompt_type=\"dynamic\", x_t=\"\"):\n        few_shot = \"\".join(\n            f\"Example:\\nContext: {ex['context']}\\nAnswer: {ex['label']} Motivation: {ex['motivation']}\\n\\n\"\n            for ex in FEW_SHOT_EXAMPLES\n        )\n        if prompt_type == \"dynamic\" and x_t:\n            return (\n                f\"Based on the following image description and technical analysis, determine whether the image is real or fake. <image> \\n\\n\"\n                f\"Image Description and Technical Analysis:\\n{x_t.strip()}\\n\\n\"\n                \"Question: Is this image real or fake? Please provide a reasoned answer based on both the description and your analysis of the image. Answer with [Real] / [Fake].\\n\"\n                \"Answer:\"  # Il modello deve rispondere con [Real] o [Fake]\n            )\n        else:\n            idx = int(prompt_type.split(\"_\")[-1]) if \"_\" in prompt_type else 0\n            template = STATIC_PROMPTS[idx]\n            return few_shot + f\"Question: {template}\\nAnswer:\"\n\n    def generate(self, image_path: str, prompt: str) -> str:\n        # 1) load+resize\n        pixel_values = load_image(image_path, max_num=12).to(torch.bfloat16).cuda()\n        generation_config = dict(max_new_tokens=1024, do_sample=True)\n        question = prompt\n        response = self.model.chat(self.tokenizer, pixel_values, question, generation_config)\n        return response\n\n    def extract_label(self, response: str) -> int:\n        return extract_label_from_response(response)\n\n    def test(self, example: dict, prompt_type=\"dynamic\"):\n        torch.cuda.empty_cache()\n        prompt = self.build_prompt(prompt_type, example.get(\"summarized_x_t\",\"\"))\n        resp = self.generate(example[\"image_k\"], prompt)\n        label = self.extract_label(resp)\n        return label, resp\n\n# wrapper\n_tester = InternVLTester()\ndef test_intern(example, prompt_type=\"dynamic\"):\n    return _tester.test(example, prompt_type)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T06:33:22.130075Z","iopub.execute_input":"2025-07-17T06:33:22.130680Z","iopub.status.idle":"2025-07-17T06:33:36.773184Z","shell.execute_reply.started":"2025-07-17T06:33:22.130657Z","shell.execute_reply":"2025-07-17T06:33:36.772530Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport pandas as pd\nfrom tqdm.auto import tqdm\nfrom datasets import load_from_disk\n\n# DATASET_PATH   = \"/kaggle/input/test-dataset-kaggle/test_dataset_kaggle\"\nCHECKPOINT_CSV = \"/kaggle/working/dynamic_new_dataset_sum_InternVL.csv\"\nPROMPT_TYPE    = \"dynamic\"\nCHECKPOINT_EVERY = 30  # salva ogni 30 campioni\n\n# 1) Carica dataset\n#dataset = load_from_disk(DATASET_PATH)\n\n# 2) Se esiste già un CSV di checkpoint, riloadalo e salta i campioni già processati\nif os.path.exists(CHECKPOINT_CSV):\n    df = pd.read_csv(CHECKPOINT_CSV)\n    processed_ids = set(df[\"img_id\"])\nelse:\n    df = pd.DataFrame(columns=[\"img_id\",\"gt_label\",\"pred_label\",\"response\"])\n    processed_ids = set()\n\n# 3) Loop con tqdm e checkpoint ogni N campioni\nfor sample in tqdm(hf_dataset, desc=f\"Eval {PROMPT_TYPE}\", dynamic_ncols=True):\n    img_id = sample[\"img_id\"]\n    if img_id in processed_ids:\n        continue\n\n    try:\n        pred_label, resp = test_intern(sample, prompt_type=PROMPT_TYPE)\n    except Exception as e:\n        print(\"Errore nella generazione\")\n        pred_label, resp = -1, f\"[ERROR] {e}\"\n\n    # prepara la nuova riga\n    row = {\n        \"img_id\":     img_id,\n        \"gt_label\":   sample[\"label\"],\n        \"pred_label\": pred_label,\n        \"response\":   resp\n    }\n    # concatena in un colpo solo\n    df = pd.concat([df, pd.DataFrame([row])], ignore_index=True)\n    processed_ids.add(img_id)\n\n    # salva checkpoint\n    if len(processed_ids) % CHECKPOINT_EVERY == 0:\n        df.to_csv(CHECKPOINT_CSV, index=False)\n\n# 4) Alla fine salva il CSV definitivo\ndf.to_csv(CHECKPOINT_CSV, index=False)\nprint(\"✅ Checkpoint salvato in:\", CHECKPOINT_CSV)\n\n# 5) Calcolo accuracy su quelli validi\nvalid = df[\"pred_label\"] != -1\nacc = (df.loc[valid, \"gt_label\"] == df.loc[valid, \"pred_label\"]).mean()\nprint(f\"Accuracy {PROMPT_TYPE}: {acc:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}