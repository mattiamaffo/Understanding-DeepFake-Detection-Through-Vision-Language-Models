{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12457166,"sourceType":"datasetVersion","datasetId":7858089},{"sourceId":12457216,"sourceType":"datasetVersion","datasetId":7858132},{"sourceId":12657249,"sourceType":"datasetVersion","datasetId":7998846},{"sourceId":12657280,"sourceType":"datasetVersion","datasetId":7998865},{"sourceId":12658472,"sourceType":"datasetVersion","datasetId":7999569},{"sourceId":12923054,"sourceType":"datasetVersion","datasetId":8177340},{"sourceId":12924296,"sourceType":"datasetVersion","datasetId":8178199},{"sourceId":12928267,"sourceType":"datasetVersion","datasetId":8180731},{"sourceId":12943779,"sourceType":"datasetVersion","datasetId":8191200}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install git+https://github.com/huggingface/transformers accelerate qwen-vl-utils bitsandbytes","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-02T18:24:41.481491Z","iopub.execute_input":"2025-09-02T18:24:41.481725Z","iopub.status.idle":"2025-09-02T18:26:39.327008Z","shell.execute_reply.started":"2025-09-02T18:24:41.481697Z","shell.execute_reply":"2025-09-02T18:26:39.326328Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import Dataset\n\n# Percorso del file salvato\n# json_path = \"/kaggle/input/final-test-set/final_dataset.json\"\ndataset_path = '/kaggle/input/summarized-dataset/dataset_summarized'\n\n#import shutil\n\n#src = \"/kaggle/input/new-correct-dataset/kaggle/working/new_correct_dataset\"\n#dst = \"/kaggle/working/test_set_correct\"\n\n#shutil.copytree(src, dst, dirs_exist_ok=True)\n#print(f\"Copiato dataset da {src} a {dst}\")\n\n# Carica il dataset Hugging Face\n# hf_dataset = Dataset.from_json(json_path)\nhf_dataset = Dataset.load_from_disk(dataset_path)\n\n# def update_image_k_path(example):\n    #old_path = example['image_k']\n    #suffix = old_path.split(\"new_data/\")[-1]\n    #new_path = f\"/kaggle/input/new-dataset-images/new_data/{suffix}\"\n    #return {\"image_k\": new_path.replace(\"\\\\\", \"/\")}\n\n#hf_dataset = hf_dataset.map(update_image_k_path)\n#hf_dataset.save_to_disk(\"/kaggle/working/new_correct_dataset_2\")\n\n# Controlla un esempio\nprint(hf_dataset)\nprint(hf_dataset['summarized_x_t'][850])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T18:27:52.536689Z","iopub.execute_input":"2025-09-02T18:27:52.537277Z","iopub.status.idle":"2025-09-02T18:27:52.550680Z","shell.execute_reply.started":"2025-09-02T18:27:52.537250Z","shell.execute_reply":"2025-09-02T18:27:52.549978Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!zip -r /kaggle/working/new_dataset_2.zip /kaggle/working/new_correct_dataset_2\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T07:19:57.656799Z","iopub.execute_input":"2025-09-01T07:19:57.657071Z","iopub.status.idle":"2025-09-01T07:19:58.113933Z","shell.execute_reply.started":"2025-09-01T07:19:57.657053Z","shell.execute_reply":"2025-09-01T07:19:58.113006Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import pipeline\n\n# Carica il classificatore zero-shot\nclassifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n\n# Etichette da prevedere\nimport re\n\nLABELS = [\"real\", \"fake\"]\n\ndef extract_label_from_response(response: str) -> int:\n    result = classifier(response, LABELS)\n    return LABELS.index(result[\"labels\"][0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T07:05:22.972743Z","iopub.execute_input":"2025-09-01T07:05:22.973433Z","iopub.status.idle":"2025-09-01T07:06:04.215324Z","shell.execute_reply.started":"2025-09-01T07:05:22.973406Z","shell.execute_reply":"2025-09-01T07:06:04.214696Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom PIL import Image\nimport torch\nfrom transformers import (\n    InstructBlipProcessor,\n    InstructBlipForConditionalGeneration,\n    GenerationConfig,\n    AddedToken,\n)\nimport bitsandbytes\nimport re\n\nos.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n\nFEW_SHOT_EXAMPLES = [\n    {\n        \"context\": \"A busy street photo at midday with natural shadows, realistic reflections, and clear faces.\",\n        \"label\": \"[Real]\"\n    },\n    {\n        \"context\": \"A misty forest with glowing edges around trees and repeating texture patterns.\",\n        \"label\": \"[Fake]\"\n    }\n]\n\n\nSTATIC_PROMPTS = [\n    \"Is this image real or fake? Choose one: [Real] / [Fake].\",\n    \"Analyze and classify: [Real] / [Fake].\",\n    \"Based on lighting, texture, and edges, decide: [Real] / [Fake].\"\n]\n\nclass BlipTester:\n    def __init__(self):\n        self.device   = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        self.model_id = \"Mediocreatmybest/instructblip-flan-t5-xl_8bit_nf4\"\n\n        # 1) Carica il modello in 8bit\n        self.model = InstructBlipForConditionalGeneration.from_pretrained(\n            self.model_id,\n            load_in_8bit=True,\n            device_map={\"\": 0},\n        )\n\n        # 2) Carica il processor\n        self.processor = InstructBlipProcessor.from_pretrained(self.model_id)\n\n        # 3) Aggiorna tokenizer per il token <image>\n        self.processor.num_query_tokens = self.model.config.num_query_tokens\n        img_tok = AddedToken(\"<image>\", normalized=False, special=True)\n        self.processor.tokenizer.add_tokens([img_tok], special_tokens=True)\n        self.model.resize_token_embeddings(len(self.processor.tokenizer), pad_to_multiple_of=64)\n        self.model.config.image_token_index = len(self.processor.tokenizer) - 1\n\n        # DEBUG\n        print(\"Image token index:\", self.model.config.image_token_index)\n\n\n    def build_prompt(self, prompt_type: str, x_t: str = \"\") -> str:\n        few_shot = \"\".join([\n            f\"Example:\\nScene Description: {ex['context']}\\nLabel: {ex['label']}\\n\\n\"\n            for ex in FEW_SHOT_EXAMPLES\n        ])\n    \n        if prompt_type == \"dynamic\" and x_t:\n            return (\n                f\"{few_shot}\"\n                f\"Scene Description: {x_t.strip()}\\n\"\n                \"Question: Is this image [Real] or [Fake]?\\n\"\n                \"Answer:\"\n            )\n        else:\n            template = STATIC_PROMPTS[int(prompt_type.split('_')[-1]) if '_' in prompt_type else 0]\n            return (\n                f\"{few_shot}\"\n                f\"Question: {template}\\nAnswer:\"\n            )\n\n\n    def generate(self, image_path: str, prompt: str) -> str:\n        # Carica e ridimensiona\n        img = Image.open(image_path).convert(\"RGB\")\n\n        # Prepara input (processor espande <image> internamente)\n        inputs = self.processor(text=prompt, images=img, truncation=True, max_length=512, padding=\"max_length\", return_tensors=\"pt\").to(self.device)\n\n        # Genera con sampling\n        outputs = self.model.generate(\n            **inputs,\n            do_sample=False,  # più deterministico\n            max_new_tokens=10, \n            eos_token_id=self.processor.tokenizer.eos_token_id,\n        )\n\n        # Decodifica\n        return self.processor.batch_decode(outputs, skip_special_tokens=True)[0].strip()\n\n    def extract_label(self, response: str) -> int:\n        return extract_label_from_response(response)\n\n    def test(self, example: dict, prompt_type: str = \"dynamic\"):\n        prompt   = self.build_prompt(prompt_type, example.get(\"summarized_x_t\", \"\"))\n        response = self.generate(example[\"image_k\"], prompt)\n        label    = self.extract_label(response)\n        return label, response\n\n# Wrapper globale\n_tester_instance = BlipTester()\ndef test_blip2(example, prompt_type=\"static_0\"):\n    torch.cuda.empty_cache()\n    return _tester_instance.test(example, prompt_type)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T07:06:38.153230Z","iopub.execute_input":"2025-09-01T07:06:38.154172Z","iopub.status.idle":"2025-09-01T07:07:35.172091Z","shell.execute_reply.started":"2025-09-01T07:06:38.154148Z","shell.execute_reply":"2025-09-01T07:07:35.171241Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import load_from_disk\nfrom tqdm import tqdm\nimport pandas as pd\n\ndef run_test():\n    #dataset = load_from_disk(DATASET_PATH)\n    #for i in range(10):\n    #print(\"Dataset structure:\\n\", hf_dataset)\n    example = hf_dataset[1]\n    print(\"Path:\\n\", example['image_k'])\n    #print(\"Image:\\n\", example['image'])\n    #print(\"Prompt:\\n\", example['x_t'])\n    label, response = test_blip2(example, \"static_0\")\n    print(\"Output label:\\n\", label)\n    print(\"Output response:\\n\", response)\n\n#run_test()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T07:20:19.691879Z","iopub.execute_input":"2025-09-01T07:20:19.692646Z","iopub.status.idle":"2025-09-01T07:20:24.228899Z","shell.execute_reply.started":"2025-09-01T07:20:19.692614Z","shell.execute_reply":"2025-09-01T07:20:24.228117Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport pandas as pd\nfrom tqdm.auto import tqdm\nfrom datasets import load_from_disk\n\n# DATASET_PATH   = \"/kaggle/input/test-dataset-kaggle/test_dataset_kaggle\"\nCHECKPOINT_CSV = \"/kaggle/working/dynamic_new_dataset_sum_InstructBlip.csv\"\nPROMPT_TYPE    = \"dynamic\"\nCHECKPOINT_EVERY = 30  # salva ogni 30 campioni\n\n# 1) Carica dataset\n#dataset = load_from_disk(DATASET_PATH)\n\n# 2) Se esiste già un CSV di checkpoint, riloadalo e salta i campioni già processati\nif os.path.exists(CHECKPOINT_CSV):\n    df = pd.read_csv(CHECKPOINT_CSV)\n    processed_ids = set(df[\"img_id\"])\nelse:\n    df = pd.DataFrame(columns=[\"img_id\",\"gt_label\",\"pred_label\",\"response\"])\n    processed_ids = set()\n\n# 3) Loop con tqdm e checkpoint ogni N campioni\nfor sample in tqdm(hf_dataset, desc=f\"Eval {PROMPT_TYPE}\", dynamic_ncols=True):\n    img_id = sample[\"img_id\"]\n    if img_id in processed_ids:\n        continue\n\n    try:\n        pred_label, resp = test_blip2(sample, prompt_type=PROMPT_TYPE)\n    except Exception as e:\n        print(\"Errore nella generazione\")\n        pred_label, resp = -1, f\"[ERROR] {e}\"\n\n    # prepara la nuova riga\n    row = {\n        \"img_id\":     img_id,\n        \"gt_label\":   sample[\"label\"],\n        \"pred_label\": pred_label,\n        \"response\":   resp\n    }\n    # concatena in un colpo solo\n    df = pd.concat([df, pd.DataFrame([row])], ignore_index=True)\n    processed_ids.add(img_id)\n\n    # salva checkpoint\n    if len(processed_ids) % CHECKPOINT_EVERY == 0:\n        df.to_csv(CHECKPOINT_CSV, index=False)\n\n# 4) Alla fine salva il CSV definitivo\ndf.to_csv(CHECKPOINT_CSV, index=False)\nprint(\"✅ Checkpoint salvato in:\", CHECKPOINT_CSV)\n\n# 5) Calcolo accuracy su quelli validi\nvalid = df[\"pred_label\"] != -1\nacc = (df.loc[valid, \"gt_label\"] == df.loc[valid, \"pred_label\"]).mean()\nprint(f\"Accuracy {PROMPT_TYPE}: {acc:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}